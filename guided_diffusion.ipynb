{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.1 torchvision==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install -r requirements.txt\n",
        "!pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co7qxgzpby3F",
        "outputId": "836a9114-10b0-4e3c-efb0-e79d889f4321"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.12.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1, 2.10.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.12.1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting mpi4py\n",
            "  Downloading mpi4py-4.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (16 kB)\n",
            "Downloading mpi4py-4.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hpourv/guided-diffusion\n",
        "%cd guided-diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ-hsROII_68",
        "outputId": "a02d318a-0529-4aa7-8316-f8a468a07f9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'guided-diffusion'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 87 (delta 3), reused 0 (delta 0), pack-reused 79 (from 1)\u001b[K\n",
            "Receiving objects: 100% (87/87), 68.20 KiB | 13.64 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "/content/guided-diffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown"
      ],
      "metadata": {
        "id": "T3qoZyjOXnmQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1gNic7OEH6pzYjBOhsIF4Yhh5FapWG6wg -O /content/guided-diffusion/ffhq_baseline.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9k4Nz8jXl_m",
        "outputId": "73261fb1-af0c-4d88-d94f-c43b569a8eb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gNic7OEH6pzYjBOhsIF4Yhh5FapWG6wg\n",
            "From (redirected): https://drive.google.com/uc?id=1gNic7OEH6pzYjBOhsIF4Yhh5FapWG6wg&confirm=t&uuid=d0e65913-0923-4d45-80ab-be165de8c961\n",
            "To: /content/guided-diffusion/ffhq_baseline.pt\n",
            "100% 374M/374M [00:02<00:00, 128MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] = os.getcwd()\n",
        "\n",
        "!python scripts/image_sample.py \\\n",
        "  --attention_resolutions 16 \\\n",
        "  --class_cond False \\\n",
        "  --diffusion_steps 1000 \\\n",
        "  --dropout 0.0 \\\n",
        "  --image_size 256 \\\n",
        "  --learn_sigma True \\\n",
        "  --noise_schedule linear \\\n",
        "  --num_channels 128 \\\n",
        "  --num_res_blocks 1 \\\n",
        "  --num_head_channels 64 \\\n",
        "  --resblock_updown True \\\n",
        "  --use_fp16 False \\\n",
        "  --use_scale_shift_norm True \\\n",
        "  --timestep_respacing 1000 \\\n",
        "  --model_path /content/guided-diffusion/ffhq_baseline.pt \\\n",
        "  --sample_dir out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGjzde9jJUD9",
        "outputId": "f0923545-a212-4cbc-a706-f7f0e7a3eba0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to out\n",
            "creating model and diffusion...\n",
            "sampling...\n",
            "created 1 / 1 samples\n",
            "saving to out/samples_1x256x256x3.npz\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "sampling complete\n",
            "[rank0]:[W130 10:49:57.263142442 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ]
    }
  ]
}